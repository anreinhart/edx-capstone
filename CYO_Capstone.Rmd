---
title: "CYO Project: Harvard PH125.9x Capstone"
author: "Ashley Reinhart"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - |
    ```{=latex}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      breaksymbolleft={}, 
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
    ```
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r loading libraries,echo=FALSE}
# install packages
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
if (!require(dplyr)) install.packages("dplyr")
library(dplyr)
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
if (!require(knitr)) install.packages("knitr")
library(knitr)
if (!require(kableExtra)) install.packages("kableExtra")
library(kableExtra)
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)
if (!require(stringr)) install.packages("stringr")
library(stringr)
if (!require(pander)) install.packages("pander")
library(pander)
if (!require(randomForest)) install.packages("randomForest")
library(randomForest)
if (!require(rpart.plot)) install.packages("rpart.plot ")
library(rpart.plot)
if (!require(rpart)) install.packages("rpart")
library(rpart)
if (!require(tibble)) install.packages("tibble")
library(tibble)
if (!require(flextable)) install.packages("flextable")
library(flextable)
```


# Introduction

This project is to satisfy the Choose Your Own portion of the HarvardX: PH125.9x Data Science Capstone course. The overall aim of this project is to demonstrate the knowledge and skills obtained from the Data Science Professional Certificate series.  

This author chose to analyse data from the National Poll on Healthy Aging's (NPHA) survey, as it directly relates to her career in the Senior Living Industry. The aim of the NPHA survey was to gather data to help inform providers, policy makers, and advocates of the various healthcare and health related issues affecting older adults. The original data set captured various responses on various topics such as health insurance, doctors visits, dental health, employment, and much more.  

# Objectives
The objectives in this analysis are to (1) test predictive models to determine which model provides the best accuracy in predicting the target variable Number.of.Doctors.Visited (2) identify the important features related to the Number.of.Doctors.Visited.

# Key Steps

This analysis will cover data exploration, additional data cleaning, modeling approaches, results, and future work recommendations. 


# Examining the Data

The NPHA data was obtained from the UCI Machine Learning Repository. The downloaded CSV file, as well as the key file can be found in this author's [GitHub repository](https://github.com/anreinhart/edx-capstone).

The data was pre-processed and only includes 14 features and one target variable from the original NPHA data set. It was further pre-processed by removing any respondents with missing responses. 

### Loading Data
```{r, loading data}
data_url <- "https://raw.githubusercontent.com/anreinhart/edx-capstone/main/NPHA-doctor-visits.csv"
NPHA_Original <- read.csv(data_url)
```


``` {r, examinig data}
str(NPHA_Original)
```

Based on the description from the UCI repository, we know the data is categorical and the values for each feature correspond to a response value. 

We can see that there are 714 observations(responses) and 15 variables (1 target; 14 features) 

A key was provided by the UCI repository/survey authors. To easily examine and plot the data, a transformed data frame will be created to replaces the variable values the corresponding string response.

### Loading the Key
```{r, loading response key}
key_url <- "https://raw.githubusercontent.com/anreinhart/edx-capstone/main/NPHA_Data_Info%20-%20Sheet1.csv"
NPHA_key <- read.csv(key_url)
```

Variable.Name|Description|Units
-----------------------------------|-----------------------|------------------------
Number.of.Doctors.Visited|The total count of different doctors the patient has seen|{1: 0-1 doctors, 2: 2-3 doctors , 3: 4 or more doctors}
Age|The patient's age group|{1: 50-64, 2: 65-80}
Physical.Health|A self-assessment of the patient's physical well-being|{-1: Refused, 1: Excellent, 2: Very Good, 3: Good, 4: Fair, 5: Poor}
Mental.Health|A self-evaluation of the patient's mental or psychological health |{-1: Refused, 1: Excellent, 2: Very Good, 3: Good, 4: Fair, 5: Poor}
Dental.Health|A self-assessment of the patient's oral or dental health|{-1: Refused, 1: Excellent, 2: Very Good, 3: Good, 4: Fair, 5: Poor}
Employment|The patient's employment status or work-related information|{-1: Refused, 1: Working full-time, 2: Working part-time, 3: Retired, 4: Not working at this time }
Stress.Keeps.Patient.from.Sleeping|Whether stress affects the patient's ability to sleep|{0: No , 1: Yes}
Medication.Keeps.Patient.from.Sleeping|Whether medication impacts the patient's sleep|{0: No , 1: Yes}
Pain.Keeps.Patient.from.Sleeping|Whether physical pain disturbs the patient's sleep|{0: No , 1: Yes}
Bathroom.Needs.Keeps.Patient.from.Sleeping|Whether the need to use the bathroom affects the patient's sleep|{0: No , 1: Yes}
Uknown.Keeps.Patient.from.Sleeping|Unidentified factors affecting the patient's sleep|{0: No , 1: Yes}
Trouble.Sleeping|General issues or difficulties the patient faces with sleeping|{0: No , 1: Yes}
Prescription.Sleep.Medication|Information about any sleep medication prescribed to the patient|{-1: Refused, 1: Use regularly, 2: Use occasionally, 3: Do not use}
Race|The patient's racial or ethnic background|{-2: Not asked, -1: REFUSED, 1: White Non-Hispanic, 2: Black Non-Hispanic, 3: Other Non-Hispanic, 4: Hispanic, 5: 2+ Races Non-Hispanic}
Gender|The gender identity of the patient|{-2: Not asked, -1: REFUSED, 1: Male, 2: Female}


A visual inspection of both data frames show a misspelling of a column in the NPHA_Original data frame. This data frame has "Phyiscal.Health", where the key is "Physical.Health" (the correct spelling). To avoid any errors in transformation, we will update the spelling in the original data frame.

# Cleaning Data
```{r, correcting Physical.Health Column }
NPHA_Original <- NPHA_Original %>%
  rename(Physical.Health = Phyiscal.Health)

str(NPHA_Original)
```
The column name is corrected. 

### Creating Data Frame with Response Values


A function to replace the numeric values with their corresponding descriptions is created to easily apply to future data frames. This function maps the values in the NPHA_Original data frame to values in the NPHA_key data frame. As visualized earlier, the values and responses in the NPHA_key data frame are combined within {} brackets. This function separates each value & response by "," then returns corresponding response on the right side of the ":". 

```{r, Function to replace numeric values with descriptions}
# function to replace numeric values with descriptions
replace_with_descriptions <- function(df, key_df) {
  for (col_name in colnames(df)) { # iterating over each column
    var_name <- col_name
    mapping <- key_df$Units[key_df$Variable.Name == var_name]  # find matching units from 'key_df' based on 'var_name'
    if (length(mapping) > 0) { # proceed if there are matches in 'key_df'
      unit_str <- key_df$Units[key_df$Variable.Name == var_name] # extract the unit string from 'key_df'
      unit_map <- strsplit(gsub("[{}]", "", unit_str), ", ")[[1]] #spliting the unit string into key-value pairs and create a named vector ('unit_map')
  # creating vector where keys are numeric values and values are corresponding labels
      unit_map <- setNames(
        sapply(unit_map, function(x) {
          key_val <- strsplit(x, ": ")[[1]] # splitting key-value pair
          as.character(key_val[2]) #extracting label (value) and convert to character
        }),
        sapply(unit_map, function(x) {
          key_val <- strsplit(x, ": ")[[1]]# extracting numeric key and convert to numeric type
          as.numeric(key_val[1])
        })
      )
      # Replace column values in 'df' with labels from 'unit_map'
      df[[col_name]] <- factor(df[[col_name]], levels = names(unit_map), labels = unname(unit_map))
    }
  }
  return(df)
}

# applying function to create new data frame
NPHA_transformed <- replace_with_descriptions(NPHA_Original, NPHA_key)

str(NPHA_transformed)
```

Inspection of the transformed data frame reveals NA values in the Dental.Health column and the Trouble.Sleeping column.

```{r}
NPHA_transformed %>%
  select(Dental.Health, Trouble.Sleeping) %>%
  slice_head(n = 8)
```


### Handeling NA Values

While the description from the UCI repository states that there are no missing values in the data set, further inspection of the NPHA_original data frame shows that the Dental.Health column includes a key value of 6, which is not an documented key value. 

The Trouble.Sleeping column contains values that do not correspond the the NPHA_key. Per the key the values for this column should be binary values of 0:No, 1:Yes. The values populating in the NPHA_Original data frame range from -1 to 3. This seemed odd, thus this author re-downloaded the data set from the repository and checked the raw CSV file to ensure data corruption did not occur. Unfotunely, values in the raw file match the incorrect values in the loaded data frame.

This author also investigated the original data source for the NPHA survey to ensure there was not a misrepresentation of the response values. According the the original data source, the response values for this column should be binary values of 0:No, 1:Yes. Since this data was pre-processed, it is possible that the original authors miss-labeled the column. 

Because NAs will pose problems for model development, they will be removed from the data set. 

Only rows containing NAs/6s in the Dental.Health column will be removed and the entire Trouble.Sleeping column will be removed since all values are corrupted. 

Prior to removing the NAs and the Trouble.Sleep column, we must ensure there are no other NAs in the data. 

First, NAs in the Dental.Health column are summed.
```{r, summing NAs for Dental.Health}
sum(is.na(NPHA_transformed$Dental.Health))
```
Check to ensure this matches the number of 6 responses in the original data frame.
```{r, summing 6s in Dental.Health}
sum(NPHA_Original$Dental.Health == 6, na.rm = TRUE)
```
Both match., which confirms there are no additional NAs are in this column. 

Now to check the number of NAs in the Trouble.Sleeping column. This is done to ensure that the combined sum of NAs in the Dental.Health and the Trouble.Sleep column match the sum of NAs for the entire data frame. If there are more NAs in the entire data frame, further investigation will be needed to determine the source. 

Summing NA's for Trouble.Sleeping
```{r, summing NAs for Trouble.Sleeping}
sum(is.na(NPHA_transformed$Trouble.Sleeping))
```
The combined sum of the two columns is 707. 

The sum of NAs for the entire transformed data frame:
```{r, checking number of NAs}
sum(is.na(NPHA_transformed))
```
707! This matches the combined sum, thus it is safe to move forward with removing NAs. 

Since this needs to be applied to both the NPHA_Original and NPHA_transformed data frames, a function is created to remove rows with NAs and 6s in the Dental.Health column, and remove the entire Trouble.Sleeping column. 

```{r, Function to remove NAs, 6s, and Trouble.Sleeping column}
remove_column_and_na_rows <- function(df, column_to_remove) {
  # check if the column_to_remove exists in the data frame
  if (column_to_remove %in% colnames(df)) {
    # remove the specified column
    df <- df[, !colnames(df) %in% column_to_remove]
  } else {
    cat(paste("Column", column_to_remove, "does not exist in the dataframe.\n"))
    return(NULL)
  }

  # filter out rows with NA values across remaining columns
  df <- df[complete.cases(df), ]

  # filter out rows where Dental.Health is 6
  df <- df[df$Dental.Health != 6, ]

  return(df)
}

#applying function to transformed dfs
NPHA_transformed <- remove_column_and_na_rows(NPHA_transformed, "Trouble.Sleeping") 

NPHA_Original <- remove_column_and_na_rows(NPHA_Original, "Trouble.Sleeping") 
```
 
The data frames have been cleaned, and now contains 659 observations (responses) and 14 variables (1 target; 13 features)

Final check to ensure all NAs are removed: 

```{r, checking na removal}
sum(is.na(NPHA_transformed))
```
All NAs removed, we can continue with our data exploration. 


# Exploratory Data Analysis 

The target variable in this data frame is Number.of.Doctors visited. 


### _Doctor Visits_
```{r, number of dr visits}
table(NPHA_transformed$Number.of.Doctors.Visited)
```
The data is imbalanced, as there are more responses in the "2-3 doctors" category and than the other two categories combined. The influence of an imbalanced data set will be further explored in the Methods and Analysis: Model section. 


### _Age & Doctor Visits_

Per the data set description observations fall within two age categories: 50-64, or 65-80. 
```{r, age and dr visits }
# create the basic bar chart
ggplot(NPHA_transformed, aes(x = factor(Number.of.Doctors.Visited), fill = Age)) +

  # add bars with dodged position and outline
  geom_bar(position = "dodge", color = NA, alpha = 0.8, width = 0.7) +

  # add text labels on top of bars (showing counts)
  geom_text(
    aes(label = stat(count)),
    stat = "count",
    position = position_dodge(width = 0.9),
    vjust = -0.5, # adjust vertical position of labels
    size = 3, # label size
    color = "black", # text color
    fontface = "bold" # label text style
  ) +

  # set fill colors for each Age group
  scale_fill_manual(
    values = c("50-64" = "#4CAF50", "65-80" = "#8C67B5"),
    name = "Age Group" # Customize legend title
  ) +
  labs(
    x = "Number of Doctor Visits",
    y = "Count",
    title = "Doctor Visits by Age Group"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 10)
  )
```
This data only contains responses from the 65-80 age group, therefore we can remove this column from the data set, as it is not relevant to this analysis. 

```{r, removing age column}
# removing age column, as there is only one age group
NPHA_Original <- subset(NPHA_Original, select = -Age)
NPHA_transformed <- subset(NPHA_transformed, select = -Age)
```


### _Gender & Race_

```{r, Plotting Gender Data}
NPHA_transformed %>%
  ggplot(aes(x = factor(Gender, levels = c("Male", "Female")), fill = Gender)) +
  # using factor to specify the order of 'Gender' levels in x-axis
  geom_bar() +
  # creating a bar plot
  scale_fill_manual(values = c("Male" = "#8dd3c7", "Female" = "#fb8072")) +
  # fill colors
  labs(x = "Gender", y = "Count", title = "Gender Distribution") +
  # Label x-axis, y-axis, and plot title
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 4, color = "white") +
  # adding count labels above bars, adjust text position and size
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  theme(legend.position = "none")
```

The data includes more responses from females than males. This is not surprising, as it is well known that on average women live longer than men. 


```{r, quick breackdown of race and responses}
table(NPHA_transformed$Race, NPHA_transformed$Gender)
```
The total number of white respondents is greater than all other races combined. 


```{r, number of visits by race and gender}
# creating custome color pallet for race categories
race_palette <- c(
  "#7BA3D0", "#F6AE2D", "#6AB187", "#F26419", "#A08BA5", "#FF7F00",
  "#4B4E6D"
)
# plotting doctor visits by race and gender with tilted x-axis labels
ggplot(NPHA_transformed, aes(x = Number.of.Doctors.Visited, fill = Race)) +
  # add bar chart with dodged bars for each Race category
  geom_bar(position = "dodge", color = NA, size = 0.2) +
  scale_fill_manual(values = race_palette) +
  # facet by Gender to create separate plots for each gender
  facet_wrap(~Gender, scales = "free") +
  labs(
    title = "Doctor Visits by Race and Gender",
    x = "Number of Doctors Visited",
    y = "Count"
  ) +
  # Customize axis and legend labels, and tilt x-axis labels
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.y = element_text(size = 12),
    axis.text.y = element_text(size = 10),
    strip.text = element_text(size = 12, face = "bold"),
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) + # Rotate x-axis labels
  # Remove legend title and customize legend appearance
  guides(fill = guide_legend(reverse = TRUE, title.position = "top", title.hjust = 0.5)) +

  # Adjust facet labels
  theme(
    strip.background = element_blank(),
    strip.placement = "outside"
  )
```
"2-3 doctors" a year is the most common among both male/female and difference races, followed by 4 or more doctors and then 0-1 doctors. We can also tell from this visual that a higher number of white non-Hispanic responses were recorded than other races. 


### _Average Ratings_
```{r, examining the data}
avg_responses <- colMeans(NPHA_Original[3:5])
# obtaining means by col
print(avg_responses)
```

A simple average of responses for each variable shows that the respondents did not rate their Mental.Health very high. Physical.Health and Dental.Health were rated higher. This data can be skewed by the different respondents. A common response, or average might be different for a different cohort (i.e. White Non-Hispanic, vs Hispanic Vs, Black Non-Hispanic, and etc.) These relationships will be further explored. 

### _Mental Health_

```{r, avg map}
avg_mental_health <- aggregate(Mental.Health ~ Race + Number.of.Doctors.Visited, data = NPHA_Original, FUN = mean)
# calculating mean of mental.health var grouped by race and number of doctor visits

#creating labels for plot
doctor_labels <- c("0-1 Doctors", "2-3 Doctors", "4 or More Doctors")
race_labels <- c("White Non-Hispanic", "Black Non-Hispanic", "Other Non-Hispanic", "Hispanic", "2+ Races Non-Hispanic")

# plotting tiled heatmap
ggplot(avg_mental_health, aes(x = factor(Race, levels = 1:5, labels = race_labels),# defining x-axis w/ custom labels
                              y = factor(Number.of.Doctors.Visited, levels = 1:3, labels = doctor_labels),  #defining y-axis w/ custom labels
                              fill = Mental.Health)) + #fill color base on avg mental.health value
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") + # color gradient
  labs(x = "Race", y = "Number of Doctors Visited", fill = "Average Mental Health Rating") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) #rotatin axis to fit
```


The Mental.Health ratings are categorized as: 

* -1: Refused

* 1: Excellent 

* 2: Very Good

* 3: Good

* 4: Fair

* 5: Poor

On average, White Non-Hispanic and Black Non-Hispanic respondents rated their mental health lower than Other Non-Hispanic, Hispanic, and 2+ Races Non-Hispanic.


### _Physical Health_
```{r, Physical health counts}

physhealth_results <- table(NPHA_transformed$Physical.Health)
physhealth_results

percentage_physhealth <- prop.table(physhealth_results) * 100
# combine categories and percentages into a data frame
results_df <- data.frame(
  Response_Category = names(physhealth_results),
  Count = as.numeric(physhealth_results), # convert counts to numeric
  Percentage = percentage_physhealth
)

print(percentage_physhealth)
```
A majority of respondents rate their physical health as Very Good or Good. 

```{r, physical health heat map for race}
avg_physical_health <- aggregate(Physical.Health ~ Race + Number.of.Doctors.Visited, data = NPHA_Original, FUN = mean)
# calculating mean of physical.health var grouped by race and number of doctor visits

# Plot heatmap
ggplot(avg_physical_health, aes(x = factor(Race, levels = 1:5, labels = race_labels), # defining x axis w/custom labels
                                y = factor(Number.of.Doctors.Visited, levels = 1:3, labels = doctor_labels), # defining y axis w/custom labels
                                fill = Physical.Health)) + #setting fill to physical.health value
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") + # color gradient
  labs(x = "Race", y = "Number of Doctors Visited", fill = "Average Physical Health Rating") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) #tilting axis to fit
```

The Physical.Health ratings are categorized as: 

* -1: Refused

* 1: Excellent 

* 2: Very Good

* 3: Good

* 4: Fair

* 5: Poor

Again, we see that Hispanic respondents are rating their Physical.Health higher than other respondents, but also seeing more doctors. An assumption is that a respondent who rates their physical health poor (or even mental or dental health) would see more doctors than someone who rates their health as < "Good". This may imply that other factors play a significant roll in the number of doctor visits. 


### _Dental Health_

```{r, dental health visual}

ggplot(NPHA_transformed, aes(x = factor(Dental.Health), fill = Race)) +

  # plot frequencies
  geom_bar(position = "dodge", stat = "count", width = 0.7) +
  scale_fill_manual(values = race_palette) +

  # plot titles and axis labels
  labs(
    title = "Dental Health Rating Distribution",
    x = "Rating",
    y = "Count of Responses",
    fill = "Race"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0, hjust = 1)) +
  guides(fill = guide_legend(reverse = TRUE)) + # reverse the order of legend keys
  coord_flip() # flip coordinates for horizontal bar plot
```
Hispanic and Black Non-Hispanic respondents rated their dental health as "Good" more frequently than other responses, where as a majority of white respondents rated their dental health as "Good", "Very Good", or "Excellent". Very few respondents of the non-white races rated their dental health as "Excellent".



### _Employment_ 


This author was interested in how respondents categorized their employment vs how they rated their mental, physical, and dental health. 

```{r, employement counts}
table(NPHA_transformed$Employment)


#calcualting retired %
Retiredpercent <- NPHA_transformed %>%
  filter(NPHA_transformed$Employment == "Retired") #filtering table to just retired 

(nrow(Retiredpercent) / length(NPHA_transformed$Employment)) * 100
```
*82 % of respondents are Retired. The remaining respondents are either working full time, part time, or not working nor retired. 

Let's view the average ratings per Physical, Mental, and Dental health by employment status. 

```{r, phys, ment, dent, by employment status}
#grouping 'NPHA_Original' by recoded 'Employment' categories and calculate averages
avg_health_by_employment <- NPHA_Original %>%
  group_by(Employment_label = recode(Employment,
    "1" = "Woirking full-time",
    "2" = "Working part-time",
    "3" = "Retired",
    "4" = "Not working at this time"
  )) %>%
  # grouping by label and for visual
  summarise(
    avg_phys_health = mean(Physical.Health, na.rm = TRUE),
    avg_ment_health = mean(Mental.Health, na.rm = TRUE),
    avg_dent_health = mean(Dental.Health, na.rm = TRUE)
  )


kable(avg_health_by_employment)
```
Respondents who listed their employment status as "Not working at this time" had, on average, higher ratings of of Physical, Mental, and Dental health. 

It is unclear in the data and response key, what the difference is between "Not working at this time" and "Retired". If directions were unclear in the original survey, it is possible respondents could have mistakenly classified themselves.

```{r, avg rating by employment status and visits}
avg_health_by_employment_visits <- NPHA_Original %>%
  mutate(Employment_label = case_when(
    Employment == 1 ~ "Working full-time",
    Employment == 2 ~ "Working part-time",
    Employment == 3 ~ "Retired",
    Employment == 4 ~ "Not working at this time"
  )) %>%
  # grouping by label and recoding for visual
  group_by(Employment_label, Number.of.Doctors.Visited) %>%
  summarise(
    avg_phys_health = mean(Physical.Health, na.rm = TRUE),
    avg_ment_health = mean(Mental.Health, na.rm = TRUE),
    avg_dent_health = mean(Dental.Health, na.rm = TRUE)
  ) %>%
  ungroup() %>% # removing grouping for further operations
  mutate(Number.of.Doctors.Visited = recode(Number.of.Doctors.Visited, !!!setNames(doctor_labels, 1:3)))


kable(avg_health_by_employment_visits)
```
Once split to view by Number.of.Doctors.Visited, the "Not working at this time" maintained a higher average than all other respondents. 

Interestingly, Retired respondents who visit "4 or more" doctors, tend to rate their mental, physical, and dental health more highly than their counterparts, 

And unsurprisingly, respondents who work full time follow an an expected trend of lower health ratings = greater number of doctor visits. 


### _Sleep_

The data on sleep includes binary responses (yes/no) to several questions regarding issues that can affect sleep. Those issues are: 

* Stress

* Medication

* Pain

* Bathroom Needs

* Unknown 


The survey also captured if a respondent utilizes prescription sleep aides with a range of:  
  
* -1: Refused

* 1: Use regularly

* 2: Use occasionally

* 3: Do not use

As aforementioned, we removed the Trouble.Sleeping question, as the values did not match the response key. 

```{r, sleep}
NPHAtrans_sleep_subset <- NPHA_transformed[, c("Stress.Keeps.Patient.from.Sleeping", "Medication.Keeps.Patient.from.Sleeping", "Pain.Keeps.Patient.from.Sleeping", "Uknown.Keeps.Patient.from.Sleeping", "Bathroom.Needs.Keeps.Patient.from.Sleeping")]
# subetting for visualizaition

# reshape data into long format
NPHASleep_long <- pivot_longer(NPHAtrans_sleep_subset,
  cols = everything(),
  names_to = "Variable",
  values_to = "Response"
)

# coounts of yes * no
sleepcounts <- table(NPHASleep_long$Variable, NPHASleep_long$Response)
sleepcounts <- data.frame(sleepcounts)


# creating custome labels for x axis
sleep_labels <- c(
  "Stress.Keeps.Patient.from.Sleeping" = "Stress",
  "Medication.Keeps.Patient.from.Sleeping" = "Medication",
  "Pain.Keeps.Patient.from.Sleeping" = "Pain",
  "Uknown.Keeps.Patient.from.Sleeping" = "Unknown",
  "Bathroom.Needs.Keeps.Patient.from.Sleeping" = "Bathroom Needs"
)

# color palette
custom_colors <- c("Yes" = "#5E9FDF", "No " = "#E6614F")


# grouped bar chart
ggplot(sleepcounts, aes(x = Var1, y = Freq, fill = Var2)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(
    title = "Counts of Yes/No by Variable Keep Patient From Sleeping",
    x = "Variables", y = "Count"
  ) +
   #customize the fill (color) scale 
  scale_fill_manual(
    values = custom_colors, name = "Response",
    labels = c("Yes" = "Yes", "No " = "No")
  ) +
  # customizing x-axis labels using predefined sleep_labels
  scale_x_discrete(labels = sleep_labels) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12, color = "black"),
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 12),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    legend.key.size = unit(1, "cm")
  ) +
  guides(fill = guide_legend(reverse = TRUE)) # reverse legend order
```
We can see that among the sleep questions, both Bathroom.Needs and Unknown.Reasons received the most "Yes" responses, but otherwise not many respondents report their sleep being affected by other sources. 


```{r, slerep medication}
table(NPHA_transformed$Prescription.Sleep.Medication)
```
And the majority of respondents do not use prescription sleep medications.



# Methods and Analysis

## _Correlation_

Since this data contains more than a 2x2 categorical variables, conventional correlation techniques like the chi square or the Pearson correlation tests will not be used. Instead to evaluate if any correlation exists between the target variable and other features Cramer's V will be utilized. Cramer's V is used to examine correlations between two categorical variables when the contingency is greater than 2x2. 

Cramer's V must lie between 0 and 1, where 0 reflects complete independence and 1 reflects complete dependence or association. ( 3 SITE)
```{r, Cramers V}
# Calculate Cramér's V for categorical variables
cramer_v <- function(x, y) {
  tab <- table(x, y)
  chisq <- chisq.test(tab)$statistic
  n <- sum(tab)
  df <- (nrow(tab) - 1) * (ncol(tab) - 1) # degrees of freedom calculation

  # Cramer's V with NaN values excluded
  if (is.nan(chisq)) {
    return(NA) # return NA if chisq is NaN
  } else {
    return(sqrt(chisq / (n * df)))
  }
}

# correlations with target variable
correlations <- sapply(NPHA_transformed[-1], function(x) {
  cramer <- cramer_v(NPHA_transformed$Number.of.Doctors.Visited, x)
  ifelse(is.na(cramer), 0, cramer) # replace NaN with 0
})


kable(correlations)
```

The factors Physical.Health and Medications.Keep.Patient.From.Sleeping seem to have a higher correlation to our target variable of Number.of.Doctors.Visited than the others. 

Due to the target variable having three levels, this author attempted to run Cramer's V on each level independently. Unfortunately, due to the low number of samples, the formula was unable to properly run. 



## _Creating Partitions_

Since the data set is not very large, it will be partitioned in a traditional 70/30 split to prevent the model from over fitting. The partitions will be created with the NPHA_Original data set to provide consistency in the models. 

```{r, creating partitions for modeling}
set.seed(123) # set seed for reproducibility

index_original <- createDataPartition(NPHA_Original$Number.of.Doctors.Visited, p = 0.7, list = FALSE)

train_original <- NPHA_Original[index_original, ] # 70% of the data for training
test_original <- NPHA_Original[-index_original, ] # Remaining 30% of the data for testing
```

## Model Seleciton


### _Random Forest_ 

The first model that will be evaluated is the Random Forest model. This model will utilize the randomForest function from the randomForest package. 
  
This model was selected because it is a supervised non-linear model that can handle categorical data without further transformation. The random forest approach is a step-above the decision tree approach, as it builds a forest of decision trees that specifies the categories with higher probability. 
  
Before deploying the model and proceeding model, we'll first convert the target variable "Number.of,Doctors.Visited" into a factor, as these are classification models and both models expect the target to be a factor. 


```{r, turning target into factor}
train_original$Number.of.Doctors.Visited <- factor(train_original$Number.of.Doctors.Visited)
test_original$Number.of.Doctors.Visited <- factor(test_original$Number.of.Doctors.Visited)
```

```{r, RF function}
# function for randomforest
fit_random_forest <- function(train_data, test_data, target_variable, to_exclude = NULL, ntree = 50000) {
  if (!is.null(to_exclude)) {
    x_train <- train_data[, -to_exclude]
    x_test <- test_data[, -to_exclude]
  } else {
    x_train <- train_data
    x_test <- test_data
  }

  y_train <- train_data[[target_variable]]

  # fitting RF
  set.seed(120) # Setting seed
  classifier_RF <- randomForest(x = x_train, y = y_train, ntree = ntree)

  # predicting on test set
  y_pred <- predict(classifier_RF, newdata = x_test)

  # confusion matrix
  confusion_mtx <- table(test_data[[target_variable]], y_pred)

  # calculate accuracy using confusion matrix
  accuracy <- sum(diag(confusion_mtx)) / sum(confusion_mtx)

  return(list(
    model = classifier_RF,
    confusion_matrix = confusion_mtx,
    accuracy = accuracy
  ))
}
```

```{r, rf function}
to_exclude <- c(1) # index of the target variable to exclude

# calling RF function
result <- fit_random_forest(
  train_data = train_original,
  test_data = test_original,
  target_variable = "Number.of.Doctors.Visited",
  to_exclude = to_exclude
)

# results
result$confusion_matrix # The confusion matrix on actual predictions
result$accuracy # The accuracy of the model
```

This model produced an accuracy of 49%. Not great, but not a bad start. Based on the confusion matrix, which displays how many of the predictions were correct vs incorrect, we can see that this version is good at predicting the "2-3 doctors" level, but it does not perform well on the other levels. 

```{r, tabeling  RF accuracy}
# create an empty tibble for model results
model_results <- tibble(
  Model = character(),
  Accuracy = numeric()
)

# updating model_results tibble
model_results <- bind_rows(model_results, tibble(
  Model = "Random Forest",
  Accuracy = result$accuracy
))

kable(model_results)
```

```{r ,rf importancel}
# importance plot
importance(result$model)

# variable importance plot
varImpPlot(result$model)
```
This model also provides the MeanDecreaseGini, which is indicative of the importance of each variable. Thus the higher the MeanDeacreseGini, the higher the relative importance.

We will further tune this model with feature selection based on the MeanDecreasGini. We can see that Physical.Health, Mental.Health, Dental.Health., Race, and Employment appear to be th most important features. These features will be isolated and tested. 


```{r, RF v2 featrure selection}
# extracting feature importance scores
feature_importance <- importance(result$model)
# extraxting names of important features based on MeanDecreaseGini values in descending order
important_features <- names(sort(feature_importance[, "MeanDecreaseGini"], decreasing = TRUE))

# selecting top n important features
n <- 5
selected_features <- important_features[1:n]


# subset the training and test data with selected features
train_data_selected <- train_original[, c("Number.of.Doctors.Visited", selected_features)]

test_data_selected <- test_original[, c("Number.of.Doctors.Visited", selected_features)]



# call RF function
result_features <- fit_random_forest(
  train_data = train_data_selected,
  test_data = test_data_selected,
  target_variable = "Number.of.Doctors.Visited",
  to_exclude = to_exclude
)

# results
result_features$model # The trained Random Forest model
result_features$confusion_matrix # The confusion matrix
result_features$accuracy # The accuracy of the model
```


The feature selections based on the MeanDecreaseGini actually decreased the accuracy of the model.

```{r, tabeling RF Feature accuracy}
# updating model_results tibble
model_results <- model_results %>%
  add_row(Model = "Random Forest w/Feature Selection", Accuracy = result_features$accuracy)

kable(model_results)
```

The next model evaluated is K Nearest Neighbors classification (KNN).  For each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. If there are ties for the kth nearest vector, all candidates are included in the vote.

K-fold cross validation will also be repeatedly applied to resample the data in an effort to prevent bias. 

### _KNN_
A function is created for the KNN model. Again, this will make it more efficient to apply this model to new data.

```{r, KNN function}
# creating function for KNN
train_and_evaluate_knn <- function(train_data, test_data, target_variable, k_values = 15:25,
                                   cv_folds = 10, cv_repeats = 3) {
  # preparing features (X) and target variable (Y) from training dataset
  X <- train_data[, !names(train_data) %in% target_variable] # features excluding target variable
  Y <- train_data[[target_variable]] # tagret variable

  # Train KNN model
  set.seed(123) # set seed for reproducibility
  knn_model <- train(X, Y,
    method = "knn", 
    trControl = trainControl( #training control for model tuning
      method = "repeatedcv", # use repeated k-fold cross-validation for model evaluation
      number = cv_folds, # folds
      repeats = cv_repeats #times to repeat CV
      ),
    tuneGrid = expand.grid(k = k_values) #specify grid k-values to tune
)
  

  # print the best tuned parameter (optimal k)
  knn_best_k <- knn_model$bestTune

  # predict on test data
  predictions <- predict(knn_model, newdata = test_data[, !names(test_data) %in% target_variable])

  #  model performance
  accuracy <- confusionMatrix(predictions, test_data[[target_variable]])$overall["Accuracy"]
  print(paste("Accuracy with KNN:", accuracy))

  # confusion matrix
  conf_matrix <- confusionMatrix(predictions, test_data[[target_variable]])

  # return the trained model and evaluation results
  return(list(model = knn_model, accuracy = accuracy, confusion_matrix = conf_matrix))
}
```


```{r, applying KNN function}
# setting the target variable name
target_variable <- "Number.of.Doctors.Visited"

# calling KNN function
result_knn <- train_and_evaluate_knn(
  train_data = train_original,
  test_data = test_original,
  target_variable = target_variable,
  k_values = 15:25,
  cv_folds = 10,
  cv_repeats = 3
)
```

```{r,  results}
result_knn$model # trained KNN model
plot(result_knn$model)
```
The most effective fold = 20. 

```{r, rsults2}
result_knn$confusion_matrix # confusion matrix of predictions
result_knn$accuracy # accuracy of the KNN model
```

Accuracy improved with KNN, with the number of neighbors selected at fold 20. Once again, we see that the model is good at predicting the "2-3 doctors category" 


```{r, tabeling KNN accuracy}
# Updating model_results tibble
model_results <- model_results %>%
  add_row(Model = "KNN", Accuracy = result_knn$accuracy)

kable(model_results)
```

Both models have done well classifying the "2-3 doctors" level. But if we remember from the earlier data exploration, this data set is imbalanced and has a much higher number of observations in the"2-3 doctors" level. Thus this author suspects the models are biased and overfitting.

## Blanaced Weights

To remove bias from the models, balanced class weights will be added. This will be done by random sampling to provide a balanced number of observations per Number.of.Doctors.Visited level, based on the smallest level. 

```{r, balanced weights}
# create manual balanced sampling
class_levels <- levels(train_original$Number.of.Doctors.Visited)
# choosing the minimum class size for balanced sampli
sample_size <- min(table(train_original$Number.of.Doctors.Visited)) # choosing the minimum class size for balanced sampling

#create empty df top store data
balanced_data <- NULL
#loop thru each class level in class_levels
for (level in class_levels) {
  #subset original data for class level
  level_data <- train_original[train_original$Number.of.Doctors.Visited == level, , drop = FALSE]
  #randomly sampling rows from subet data & combining 
  balanced_data <- rbind(balanced_data, level_data[sample(1:nrow(level_data), sample_size), ])
}

# shuffle the balanced data
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]

# confirm new distribution
table(balanced_data$Number.of.Doctors.Visited)
```

The training set is now balanced. This is only performed on the training set, as the aim of the test set is to mimic real world data, where balance will not necessarily occur. 


Now the models will be trained on the balanced_data set.

### _Balanced Weights: Random Forest_

```{r, RF function balanced}
to_exclude <- c(1) # index of the target variable to exclude

# calling function
result_RF_balanced <- fit_random_forest(
  train_data = balanced_data,
  test_data = test_original,
  target_variable = "Number.of.Doctors.Visited",
  to_exclude = to_exclude
)

# results
result_RF_balanced$model # trained Random Forest model
result_RF_balanced$confusion_matrix # confusion matrix
result_RF_balanced$accuracy # accuracy of the model


# importance plot
importance(result_RF_balanced$model)

# variable importance plot
varImpPlot(result_RF_balanced$model)
```

We see a decrease in accuracy, which confirms that the first iteration of the model was biased towards the "2-3 doctors" observations. Important features did not change from the initial model. These will now be added in and trained on the balanced data. 


```{r, tabeling RF BALANCED accuracy}
# Updating the model_results tibble to add a new column
model_results <- model_results %>%
  mutate(`Accuracy w/Balanced Weights` = NA_real_) # Initialize a new column with NA

# assign  accuracy value for "Random Forest" rows in the new column
model_results$`Accuracy w/Balanced Weights`[model_results$Model == "Random Forest"] <- result_RF_balanced$accuracy

# Print
kable(model_results)
```


### _Balanced Weights: Random Forest w/Feature Selection_
```{r, RF v2 balanced feature selection}
# extracting feature importance scores
feature_importance <- importance(result_RF_balanced$model)

important_features <- names(sort(feature_importance[, "MeanDecreaseGini"], decreasing = TRUE))

# selecting top n important features
n <- 5
selected_features <- important_features[1:n]


# subset the training and test data with selected features
train_data_selected <- balanced_data[, c("Number.of.Doctors.Visited", selected_features)]

test_data_selected <- test_original[, c("Number.of.Doctors.Visited", selected_features)]



# calling function
result_features_balanced <- fit_random_forest(
  train_data = train_data_selected,
  test_data = test_data_selected,
  target_variable = "Number.of.Doctors.Visited",
  to_exclude = to_exclude
)

# results
result_features_balanced$model # trained Random Forest model
result_features_balanced$confusion_matrix #  confusion matrix
result_features_balanced$accuracy # accuracy of the model
```

With a balanced training set, the random forest model accuracy has improved with feature selection, which we did not see previously. 

```{r, tabeling RF Feature Balanced results}
# add the second balanced weight value for the "Random Forest" model
# assign the accuracy value for "Random Forest" rows in the new column
model_results$`Accuracy w/Balanced Weights`[model_results$Model == "Random Forest w/Feature Selection"] <- result_features_balanced$accuracy

# Print updated model_results tibble
print(model_results)
```

### _Balanced Weights: KNN_

Finally we test on the KNN model.

```{r, KNN balanced}
# setting target variable
target_variable <- "Number.of.Doctors.Visited"

# calling function
result_knn_balanced <- train_and_evaluate_knn(
  train_data = balanced_data,
  test_data = test_original,
  target_variable = target_variable,
  k_values = 15:25,
  cv_folds = 10,
  cv_repeats = 3
)

# results
result_knn_balanced$model # trained KNN model
result_knn_balanced$accuracy # accuracy of the KNN model
result_knn_balanced$confusion_matrix # confusion matrix of prediction

plot(result_knn_balanced$model)
```

Applying the balanced data to the KNN model shows a slight increase in accuracy, and an increase in the K-fold value used for the model to 23. Thus we have found a sufficient model to continue future work on. 


```{r, tabeling KNN Balanced results}
# adding the balanced weight for KNN
# assign the accuracy value for "KNN" rows in the new column
model_results$`Accuracy w/Balanced Weights`[model_results$Model == "KNN"] <- result_knn_balanced$accuracy

# Print
kable(model_results)
```



# Conclusion

The KNN with a balanced training model data preformed the best. Key features that may relate to the target variable Number.of.Doctors.Visited included, Physical.Health, Mental.Health, Dental.Health, and Employment. While accuracy of this model was not above 50%, this does provide a starting point for future work. 

Future work should include a larger subset of the original survey data, as this author believes the small portion of features does not tell the whole story behind number of doctors visited. Factors such as socioeconomic status, medical insurance, geographic location, and etc., all contribute to the complexity of healthcare decisions and utilization. Likewise, an entire column (Trouble.Sleeping) had to be removed because of incorrect data. This author considered creating an additional column that provided a similar metric (i.e. if any of the preceding sleep questions = 1, then return 1 for sleep troubles, otherwise 0). This author decided against this step to avoid assumptions, as a respondent could theoretically answer no to all the sleep questions, but still answer yes to Trouble.Sleeping.

Future work on the models themselves should include additional tuning and parameter selection, for example, the Random Forest method could have specified the number of variables to randomly sample(mtry) at each split, and/or tested larger forests.This author was limited by her computer, as it could not process a forest greater than 50000 trees. Further work should be done on a system that can handle large complex data. Additional models should also be investigated, such as XGBoost, which is a distributed gradient boosting library, that combines predictions of multiple weak models to produce strong predictions. This analysis is limited to only Random Forest and KNN methods, and additional methods may prove to be more accurate. 


# References 

* Malani, Preeti N., Kullgren, Jeffrey, and Solway, Erica. National Poll on Healthy Aging (NPHA), [United States], April 2017.Inter-university Consortium for Political and Social Research [distributor], 2019-05-29. https://doi.org/10.3886/ICPSR37305.v1

* Ginter E, Simko V. Women live longer than men. Bratisl Lek Listy. 2013;114(2):45-9. doi: 10.4149/bll_2013_011. PMID: 23331196.

* “Lectur15.” Web.pdx.edu, web.pdx.edu/~newsomj/pa551/lectur15.htm#:~:text=Cramer.

* “Random Forest Approach for Classification in R Programming.” GeeksforGeeks, 5 July 2020, www.geeksforgeeks.org/random-forest-approach-for-classification-in-r-programming/#. Accessed 15 Apr. 2024.

* “Machine Learning - Random Forest - Q.” Wiki.q-Researchsoftware.com, 20 Oct. 2023, wiki.q-researchsoftware.com/wiki/Machine_Learning_-_Random_Forest. Accessed 15 Apr. 2024.

* “Knn Function - RDocumentation.” Www.rdocumentation.org,www.rdocumentation.org/packages/class/versions/7.3-22/topics/knn.

* “XGBoost.” GeeksforGeeks, 18 Sept. 2021, www.geeksforgeeks.org/xgboost/#. Accessed 15 Apr. 2024.
